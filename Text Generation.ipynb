{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Assi(5) - Njood.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"WbFnbfw8Rmne","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624120958453,"user_tz":-180,"elapsed":3412780,"user":{"displayName":"Njood Aljasser","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNdi6NtS9I8yHqJkqKcvVMHGOkSI_FwPS9TumABQ=s64","userId":"09262083240234945278"}},"outputId":"94c84426-b242-406a-d9e4-99cd0c80b4c8"},"source":["# Small LSTM Network to Generate Text\n","import numpy\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import Dropout\n","from keras.layers import LSTM\n","from keras.callbacks import ModelCheckpoint\n","from keras.utils import np_utils\n","# load ascii text and covert to lowercase\n","filename = \"/content/834-0.txt\"\n","raw_text = open(filename, 'r', encoding='utf-8').read()\n","raw_text = raw_text.lower()\n","# create mapping of unique chars to integers\n","chars = sorted(list(set(raw_text)))\n","char_to_int = dict((c, i) for i, c in enumerate(chars))\n","# summarize the loaded data\n","n_chars = len(raw_text)\n","n_vocab = len(chars)\n","print (\"Total Characters: \", n_chars)\n","print (\"Total Vocab: \", n_vocab)\n","# prepare the dataset of input to output pairs encoded as integers\n","seq_length = 100\n","dataX = []\n","dataY = []\n","for i in range(0, n_chars - seq_length, 1):\n","\tseq_in = raw_text[i:i + seq_length]\n","\tseq_out = raw_text[i + seq_length]\n","\tdataX.append([char_to_int[char] for char in seq_in])\n","\tdataY.append(char_to_int[seq_out])\n","n_patterns = len(dataX)\n","print (\"Total Patterns: \", n_patterns)\n","# reshape X to be [samples, time steps, features]\n","X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n","# normalize\n","X = X / float(n_vocab)\n","# one hot encode the output variable\n","y = np_utils.to_categorical(dataY)\n","# define the LSTM model\n","model = Sequential()\n","model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n","model.add(Dropout(0.2))\n","model.add(Dense(y.shape[1], activation='softmax'))\n","model.compile(loss='categorical_crossentropy', optimizer='adam')\n","# define the checkpoint\n","filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n","checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n","callbacks_list = [checkpoint]\n","# fit the model\n","model.fit(X, y, epochs=50, batch_size=128, callbacks=callbacks_list)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Total Characters:  588955\n","Total Vocab:  73\n","Total Patterns:  588855\n","Epoch 1/50\n","4601/4601 [==============================] - 68s 14ms/step - loss: 2.7726\n","\n","Epoch 00001: loss improved from inf to 2.63074, saving model to weights-improvement-01-2.6307.hdf5\n","Epoch 2/50\n","4601/4601 [==============================] - 68s 15ms/step - loss: 2.4821\n","\n","Epoch 00002: loss improved from 2.63074 to 2.45972, saving model to weights-improvement-02-2.4597.hdf5\n","Epoch 3/50\n","4601/4601 [==============================] - 67s 15ms/step - loss: 2.4140\n","\n","Epoch 00003: loss improved from 2.45972 to 2.40141, saving model to weights-improvement-03-2.4014.hdf5\n","Epoch 4/50\n","4601/4601 [==============================] - 68s 15ms/step - loss: 2.3563\n","\n","Epoch 00004: loss improved from 2.40141 to 2.34302, saving model to weights-improvement-04-2.3430.hdf5\n","Epoch 5/50\n","4601/4601 [==============================] - 68s 15ms/step - loss: 2.2922\n","\n","Epoch 00005: loss improved from 2.34302 to 2.28106, saving model to weights-improvement-05-2.2811.hdf5\n","Epoch 6/50\n","4601/4601 [==============================] - 68s 15ms/step - loss: 2.2365\n","\n","Epoch 00006: loss improved from 2.28106 to 2.22578, saving model to weights-improvement-06-2.2258.hdf5\n","Epoch 7/50\n","4601/4601 [==============================] - 68s 15ms/step - loss: 2.1864\n","\n","Epoch 00007: loss improved from 2.22578 to 2.17673, saving model to weights-improvement-07-2.1767.hdf5\n","Epoch 8/50\n","4601/4601 [==============================] - 69s 15ms/step - loss: 2.1415\n","\n","Epoch 00008: loss improved from 2.17673 to 2.13428, saving model to weights-improvement-08-2.1343.hdf5\n","Epoch 9/50\n","4601/4601 [==============================] - 69s 15ms/step - loss: 2.1057\n","\n","Epoch 00009: loss improved from 2.13428 to 2.09756, saving model to weights-improvement-09-2.0976.hdf5\n","Epoch 10/50\n","4601/4601 [==============================] - 68s 15ms/step - loss: 2.0743\n","\n","Epoch 00010: loss improved from 2.09756 to 2.07079, saving model to weights-improvement-10-2.0708.hdf5\n","Epoch 11/50\n","4601/4601 [==============================] - 68s 15ms/step - loss: 2.0460\n","\n","Epoch 00011: loss improved from 2.07079 to 2.04488, saving model to weights-improvement-11-2.0449.hdf5\n","Epoch 12/50\n","4601/4601 [==============================] - 68s 15ms/step - loss: 2.0217\n","\n","Epoch 00012: loss improved from 2.04488 to 2.01983, saving model to weights-improvement-12-2.0198.hdf5\n","Epoch 13/50\n","4601/4601 [==============================] - 68s 15ms/step - loss: 1.9994\n","\n","Epoch 00013: loss improved from 2.01983 to 1.99805, saving model to weights-improvement-13-1.9980.hdf5\n","Epoch 14/50\n","4601/4601 [==============================] - 68s 15ms/step - loss: 1.9769\n","\n","Epoch 00014: loss improved from 1.99805 to 1.97559, saving model to weights-improvement-14-1.9756.hdf5\n","Epoch 15/50\n","4601/4601 [==============================] - 68s 15ms/step - loss: 1.9562\n","\n","Epoch 00015: loss improved from 1.97559 to 1.95697, saving model to weights-improvement-15-1.9570.hdf5\n","Epoch 16/50\n","4601/4601 [==============================] - 68s 15ms/step - loss: 1.9382\n","\n","Epoch 00016: loss improved from 1.95697 to 1.93895, saving model to weights-improvement-16-1.9389.hdf5\n","Epoch 17/50\n","4601/4601 [==============================] - 68s 15ms/step - loss: 1.9206\n","\n","Epoch 00017: loss improved from 1.93895 to 1.92379, saving model to weights-improvement-17-1.9238.hdf5\n","Epoch 18/50\n","4601/4601 [==============================] - 68s 15ms/step - loss: 1.9040\n","\n","Epoch 00018: loss improved from 1.92379 to 1.90902, saving model to weights-improvement-18-1.9090.hdf5\n","Epoch 19/50\n","4601/4601 [==============================] - 68s 15ms/step - loss: 1.8914\n","\n","Epoch 00019: loss improved from 1.90902 to 1.89476, saving model to weights-improvement-19-1.8948.hdf5\n","Epoch 20/50\n","4601/4601 [==============================] - 68s 15ms/step - loss: 1.8775\n","\n","Epoch 00020: loss improved from 1.89476 to 1.88328, saving model to weights-improvement-20-1.8833.hdf5\n","Epoch 21/50\n","4601/4601 [==============================] - 68s 15ms/step - loss: 1.8683\n","\n","Epoch 00021: loss improved from 1.88328 to 1.87171, saving model to weights-improvement-21-1.8717.hdf5\n","Epoch 22/50\n","4601/4601 [==============================] - 68s 15ms/step - loss: 1.8572\n","\n","Epoch 00022: loss improved from 1.87171 to 1.86115, saving model to weights-improvement-22-1.8611.hdf5\n","Epoch 23/50\n","4601/4601 [==============================] - 68s 15ms/step - loss: 1.8455\n","\n","Epoch 00023: loss improved from 1.86115 to 1.84935, saving model to weights-improvement-23-1.8494.hdf5\n","Epoch 24/50\n","4601/4601 [==============================] - 68s 15ms/step - loss: 1.8351\n","\n","Epoch 00024: loss improved from 1.84935 to 1.84066, saving model to weights-improvement-24-1.8407.hdf5\n","Epoch 25/50\n","4601/4601 [==============================] - 68s 15ms/step - loss: 1.8278\n","\n","Epoch 00025: loss improved from 1.84066 to 1.83381, saving model to weights-improvement-25-1.8338.hdf5\n","Epoch 26/50\n","4601/4601 [==============================] - 68s 15ms/step - loss: 1.8198\n","\n","Epoch 00026: loss improved from 1.83381 to 1.82543, saving model to weights-improvement-26-1.8254.hdf5\n","Epoch 27/50\n","4601/4601 [==============================] - 68s 15ms/step - loss: 1.8159\n","\n","Epoch 00027: loss improved from 1.82543 to 1.81959, saving model to weights-improvement-27-1.8196.hdf5\n","Epoch 28/50\n","4601/4601 [==============================] - 68s 15ms/step - loss: 1.8014\n","\n","Epoch 00028: loss improved from 1.81959 to 1.80755, saving model to weights-improvement-28-1.8075.hdf5\n","Epoch 29/50\n","4601/4601 [==============================] - 68s 15ms/step - loss: 1.7957\n","\n","Epoch 00029: loss improved from 1.80755 to 1.80292, saving model to weights-improvement-29-1.8029.hdf5\n","Epoch 30/50\n","4601/4601 [==============================] - 68s 15ms/step - loss: 1.7890\n","\n","Epoch 00030: loss improved from 1.80292 to 1.79429, saving model to weights-improvement-30-1.7943.hdf5\n","Epoch 31/50\n","4601/4601 [==============================] - 68s 15ms/step - loss: 1.7818\n","\n","Epoch 00031: loss improved from 1.79429 to 1.78788, saving model to weights-improvement-31-1.7879.hdf5\n","Epoch 32/50\n","4601/4601 [==============================] - 68s 15ms/step - loss: 1.7823\n","\n","Epoch 00032: loss improved from 1.78788 to 1.78358, saving model to weights-improvement-32-1.7836.hdf5\n","Epoch 33/50\n","4601/4601 [==============================] - 68s 15ms/step - loss: 1.7697\n","\n","Epoch 00033: loss improved from 1.78358 to 1.77734, saving model to weights-improvement-33-1.7773.hdf5\n","Epoch 34/50\n","4601/4601 [==============================] - 68s 15ms/step - loss: 1.7652\n","\n","Epoch 00034: loss improved from 1.77734 to 1.77337, saving model to weights-improvement-34-1.7734.hdf5\n","Epoch 35/50\n","4601/4601 [==============================] - 68s 15ms/step - loss: 1.7585\n","\n","Epoch 00035: loss improved from 1.77337 to 1.76694, saving model to weights-improvement-35-1.7669.hdf5\n","Epoch 36/50\n","4601/4601 [==============================] - 68s 15ms/step - loss: 1.7582\n","\n","Epoch 00036: loss did not improve from 1.76694\n","Epoch 37/50\n","4601/4601 [==============================] - 68s 15ms/step - loss: 1.7526\n","\n","Epoch 00037: loss improved from 1.76694 to 1.75886, saving model to weights-improvement-37-1.7589.hdf5\n","Epoch 38/50\n","4601/4601 [==============================] - 68s 15ms/step - loss: 1.7500\n","\n","Epoch 00038: loss improved from 1.75886 to 1.75281, saving model to weights-improvement-38-1.7528.hdf5\n","Epoch 39/50\n","4601/4601 [==============================] - 68s 15ms/step - loss: 1.9276\n","\n","Epoch 00039: loss did not improve from 1.75281\n","Epoch 40/50\n","4601/4601 [==============================] - 68s 15ms/step - loss: 2.1487\n","\n","Epoch 00040: loss did not improve from 1.75281\n","Epoch 41/50\n","4601/4601 [==============================] - 68s 15ms/step - loss: 1.8021\n","\n","Epoch 00041: loss did not improve from 1.75281\n","Epoch 42/50\n","4601/4601 [==============================] - 68s 15ms/step - loss: 1.7476\n","\n","Epoch 00042: loss did not improve from 1.75281\n","Epoch 43/50\n","4601/4601 [==============================] - 68s 15ms/step - loss: 1.7423\n","\n","Epoch 00043: loss improved from 1.75281 to 1.74831, saving model to weights-improvement-43-1.7483.hdf5\n","Epoch 44/50\n","4601/4601 [==============================] - 68s 15ms/step - loss: 1.7300\n","\n","Epoch 00044: loss improved from 1.74831 to 1.74055, saving model to weights-improvement-44-1.7405.hdf5\n","Epoch 45/50\n","4601/4601 [==============================] - 68s 15ms/step - loss: 1.7298\n","\n","Epoch 00045: loss improved from 1.74055 to 1.73595, saving model to weights-improvement-45-1.7359.hdf5\n","Epoch 46/50\n","4601/4601 [==============================] - 68s 15ms/step - loss: 1.7265\n","\n","Epoch 00046: loss improved from 1.73595 to 1.73153, saving model to weights-improvement-46-1.7315.hdf5\n","Epoch 47/50\n","4601/4601 [==============================] - 68s 15ms/step - loss: 1.7194\n","\n","Epoch 00047: loss improved from 1.73153 to 1.72520, saving model to weights-improvement-47-1.7252.hdf5\n","Epoch 48/50\n","4601/4601 [==============================] - 68s 15ms/step - loss: 1.7148\n","\n","Epoch 00048: loss improved from 1.72520 to 1.72169, saving model to weights-improvement-48-1.7217.hdf5\n","Epoch 49/50\n","4601/4601 [==============================] - 68s 15ms/step - loss: 1.7146\n","\n","Epoch 00049: loss improved from 1.72169 to 1.72085, saving model to weights-improvement-49-1.7209.hdf5\n","Epoch 50/50\n","4601/4601 [==============================] - 68s 15ms/step - loss: 1.7147\n","\n","Epoch 00050: loss improved from 1.72085 to 1.71481, saving model to weights-improvement-50-1.7148.hdf5\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fef6c1334d0>"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"nli5axESZSfr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624121438753,"user_tz":-180,"elapsed":47774,"user":{"displayName":"Njood Aljasser","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNdi6NtS9I8yHqJkqKcvVMHGOkSI_FwPS9TumABQ=s64","userId":"09262083240234945278"}},"outputId":"4ca0567a-5ee6-4ac0-d1f6-ebe1c3eac711"},"source":["import sys\n","import numpy\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import Dropout\n","from keras.layers import LSTM\n","from keras.callbacks import ModelCheckpoint\n","from keras.utils import np_utils\n","# load ascii text and covert to lowercase\n","filename = \"/content/834-0.txt\"\n","raw_text = open(filename, 'r', encoding='utf-8').read()\n","raw_text = raw_text.lower()\n","# create mapping of unique chars to integers, and a reverse mapping\n","chars = sorted(list(set(raw_text)))\n","char_to_int = dict((c, i) for i, c in enumerate(chars))\n","int_to_char = dict((i, c) for i, c in enumerate(chars))\n","# summarize the loaded data\n","n_chars = len(raw_text)\n","n_vocab = len(chars)\n","print (\"Total Characters: \", n_chars)\n","print (\"Total Vocab: \", n_vocab)\n","# prepare the dataset of input to output pairs encoded as integers\n","seq_length = 100\n","dataX = []\n","dataY = []\n","for i in range(0, n_chars - seq_length, 1):\n","\tseq_in = raw_text[i:i + seq_length]\n","\tseq_out = raw_text[i + seq_length]\n","\tdataX.append([char_to_int[char] for char in seq_in])\n","\tdataY.append(char_to_int[seq_out])\n","n_patterns = len(dataX)\n","print (\"Total Patterns: \", n_patterns)\n","# reshape X to be [samples, time steps, features]\n","X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n","# normalize\n","X = X / float(n_vocab)\n","# one hot encode the output variable\n","y = np_utils.to_categorical(dataY)\n","# define the LSTM model\n","model = Sequential()\n","model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n","model.add(Dropout(0.2))\n","model.add(Dense(y.shape[1], activation='softmax'))\n","# load the network weights\n","filename = \"/content/weights-improvement-50-1.7148.hdf5\"\n","model.load_weights(filename)\n","model.compile(loss='categorical_crossentropy', optimizer='adam')\n","# pick a random seed\n","start = numpy.random.randint(0, len(dataX)-1)\n","pattern = dataX[start]\n","print (\"Seed:\")\n","print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n","# generate characters\n","for i in range(1000):\n","\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n","\tx = x / float(n_vocab)\n","\tprediction = model.predict(x, verbose=0)\n","\tindex = numpy.argmax(prediction)\n","\tresult = int_to_char[index]\n","\tseq_in = [int_to_char[value] for value in pattern]\n","\tsys.stdout.write(result)\n","\tpattern.append(index)\n","\tpattern = pattern[1:len(pattern)]\n","print (\"\\nDone.\")"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Total Characters:  588955\n","Total Vocab:  73\n","Total Patterns:  588855\n","Seed:\n","\" ow\n","      and the rudeness of the woman. i determined to say nothing about\n","      the former to my wif \"\n","e to the soom of the mant that i had been\n","      sore to hev to the same of the saaee of the saale. the taale hn\n","      the saale of the saale of the saaee of the saaee of the       corpanirg of the saaee of the soom and the saaee of the saale       cound he was a sereer were on the saale of the saale. the tas       that he was a sereer warte of the saale of the saale of the\n","      sooet of the saale of the saale of the saaee of the saale and       couter of the saale of the soom and the saaee of the saale and\n","      couter of the saale of the soom and the saaee of the saale and\n","      couter of the saale of the soom and the saaee of the saale and\n","      couter of the saale of the soom and the saaee of the saale and\n","      couter of the saale of the soom and the saaee of the saale and\n","      couter of the saale of the soom and the saaee of the saale and\n","      couter of the saale of the soom and the saaee of the saale and\n","      couter of the saale of the soom and the saaee of the saale and\n","    \n","Done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Sep6UrhvpsE2"},"source":[""],"execution_count":null,"outputs":[]}]}